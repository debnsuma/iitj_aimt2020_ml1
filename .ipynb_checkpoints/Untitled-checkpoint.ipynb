{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make the imports of python packages needed\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "#Import the dataset and define the feature as well as the target datasets / columns#\n",
    "file_name = \"wifi_localization.txt\"\n",
    "dataset = pd.read_csv(file_name, delimiter=\"\\t\", header=None)\n",
    "\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "dataset.columns = [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"class_label\"]\n",
    "###################\n",
    "\n",
    "\n",
    "\n",
    "def entropy(target_col):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "    The only parameter of this function is the target_col parameter which specifies the target column\n",
    "    \"\"\"\n",
    "    elements,counts = np.unique(target_col,return_counts = True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "\n",
    "################### \n",
    "    \n",
    "###################\n",
    "\n",
    "\n",
    "def InfoGain(data,split_attribute_name,target_name=\"class_label\"):\n",
    "    \"\"\"\n",
    "    Calculate the information gain of a dataset. This function takes three parameters:\n",
    "    1. data = The dataset for whose feature the IG should be calculated\n",
    "    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n",
    "    3. target_name = the name of the target feature. The default for this example is \"class\"\n",
    "    \"\"\"    \n",
    "    #Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    ##Calculate the entropy of the dataset\n",
    "    \n",
    "    #Calculate the values and the corresponding counts for the split attribute \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "    \n",
    "    #Calculate the weighted entropy\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain\n",
    "       \n",
    "###################\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "def ID3(data,originaldata,features,target_attribute_name=\"class_label\",parent_node_class = None):\n",
    "    \"\"\"\n",
    "    ID3 Algorithm: This function takes five paramters:\n",
    "    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n",
    " \n",
    "    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n",
    "    in the case the dataset delivered by the first parameter is empty\n",
    "\n",
    "    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n",
    "    we have to remove features from our dataset --> Splitting at each node\n",
    "\n",
    "    4. target_attribute_name = the name of the target attribute\n",
    "\n",
    "    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n",
    "    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n",
    "    space, we want to return the mode target feature value of the direct parent node.\n",
    "    \"\"\"   \n",
    "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
    "    \n",
    "    #If all target_values have the same value, return this value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
    "    elif len(data)==0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "    \n",
    "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
    "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
    "    #the mode target feature value is stored in the parent_node_class variable.\n",
    "    \n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    #If none of the above holds true, grow the tree!\n",
    "    \n",
    "    else:\n",
    "        #Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        #Select the feature which best splits the dataset\n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
    "        #gain in the first run\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        \n",
    "        #Remove the feature with the best inforamtion gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        #Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = ID3(sub_data,originaldata,features,target_attribute_name,parent_node_class)\n",
    "            \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return tree    \n",
    "                \n",
    "###################\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def predict(query,tree,default = 1):\n",
    "    \"\"\"\n",
    "    Prediction of a new/unseen query instance. This takes two parameters:\n",
    "    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n",
    "\n",
    "    2. The tree \n",
    "\n",
    "\n",
    "    We do this also in a recursive manner. That is, we wander down the tree and check if we have reached a leaf or if we are still in a sub tree. \n",
    "    Since this is a important step to understand, the single steps are extensively commented below.\n",
    "\n",
    "    1.Check for every feature in the query instance if this feature is existing in the tree.keys() for the first call, \n",
    "    tree.keys() only contains the value for the root node \n",
    "    --> if this value is not existing, we can not make a prediction and have to \n",
    "    return the default value which is the majority value of the target feature\n",
    "\n",
    "    2. First of all we have to take care of a important fact: Since we train our model with a database A and then show our model\n",
    "    a unseen query it may happen that the feature values of these query are not existing in our tree model because non of the\n",
    "    training instances has had such a value for this specific feature. \n",
    "    For instance imagine the situation where your model has only seen animals with one to four\n",
    "    legs - The \"legs\" node in your model will only have four outgoing branches (from one to four). If you now show your model\n",
    "    a new instance (animal) which has for the legs feature the vale 5, you have to tell your model what to do in such a \n",
    "    situation because otherwise there is no classification possible because in the classification step you try to \n",
    "    run down the outgoing branch with the value 5 but there is no such a branch. Hence: Error and no Classification!\n",
    "    We can address this issue with a classification value of for instance (999) which tells us that there is no classification\n",
    "    possible or we assign the most frequent target feature value of our dataset used to train the model. Or, in for instance \n",
    "    medical application we can return the most worse case - just to make sure... \n",
    "    We can also return the most frequent value of the direct parent node. To make a long story short, we have to tell the model \n",
    "    what to do in this situation.\n",
    "    In our example, since we are dealing with animal species where a false classification is not that critical, we will assign\n",
    "    the value 1 which is the value for the mammal species (for convenience).\n",
    "\n",
    "    3. Address the key in the tree which fits the value for key --> Note that key == the features in the query. \n",
    "    Because we want the tree to predict the value which is hidden under the key value (imagine you have a drawn tree model on \n",
    "    the table in front of you and you have a query instance for which you want to predict the target feature \n",
    "    - What are you doing? - Correct:\n",
    "    You start at the root node and wander down the tree comparing your query to the node values. Hence you want to have the\n",
    "    value which is hidden under the current node. If this is a leaf, perfect, otherwise you wander the tree deeper until you\n",
    "    get to a leaf node. \n",
    "    Though, you want to have this \"something\" [either leaf or sub_tree] which is hidden under the current node\n",
    "    and hence we must address the node in the tree which == the key value from our query instance. \n",
    "    This is done with tree[keys]. Next you want to run down the branch of this node which is equal to the value given \"behind\"\n",
    "    the key value of your query instance e.g. if you find \"legs\" == to tree.keys() that is, for the first run == the root node.\n",
    "    You want to run deeper and therefore you have to address the branch at your node whose value is == to the value behind key.\n",
    "    This is done with query[key] e.g. query[key] == query['legs'] == 0 --> Therewith we run down the branch of the node with the\n",
    "    value 0. Summarized, in this step we want to address the node which is hidden behind a specific branch of the root node (in the first run)\n",
    "    this is done with: result = [key][query[key]]\n",
    "\n",
    "    4. As said in the 2. step, we run down the tree along nodes and branches until we get to a leaf node.\n",
    "    That is, if result = tree[key][query[key]] returns another tree object (we have represented this by a dict object --> \n",
    "    that is if result is a dict object) we know that we have not arrived at a root node and have to run deeper the tree. \n",
    "    Okay... Look at your drawn tree in front of you... what are you doing?...well, you run down the next branch... \n",
    "    exactly as we have done it above with the slight difference that we already have passed a node and therewith \n",
    "    have to run only a fraction of the tree --> You clever guy! That \"fraction of the tree\" is exactly what we have stored\n",
    "    under 'result'.\n",
    "    So we simply call our predict method using the same query instance (we do not have to drop any features from the query\n",
    "    instance since for instance the feature for the root node will not be available in any of the deeper sub_trees and hence \n",
    "    we will simply not find that feature) as well as the \"reduced / sub_tree\" stored in result.\n",
    "\n",
    "    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n",
    "    name of the root node is equal to one of the query features.\n",
    "    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n",
    "    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n",
    "    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n",
    "    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n",
    "    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n",
    "    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n",
    "    below this node and do not run the whole tree beginning at the root node \n",
    "    --> This is why we re-call the classification function with 'result'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #1.\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            #2.\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "  \n",
    "            #3.\n",
    "            result = tree[key][query[key]]\n",
    "            #4.\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "Check the accuracy of our prediction.\n",
    "The train_test_split function takes the dataset as parameter which should be divided into\n",
    "a training and a testing set. The test function takes two parameters, which are the testing data as well as the tree model.\n",
    "\"\"\"\n",
    "###################\n",
    "\n",
    "###################\n",
    "\n",
    "def train_test_split(dataset, split_ratio=.80):\n",
    "    \n",
    "    total_rows = dataset.shape[0]\n",
    "    split = int(split_ratio * total_rows)\n",
    "    \n",
    "    training_data = dataset.iloc[:split].reset_index(drop=True)    \n",
    "    testing_data = dataset.iloc[split:].reset_index(drop=True)\n",
    "    \n",
    "    return training_data,testing_data\n",
    "\n",
    "\n",
    "def test(data,tree):\n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
    "    #convert it to a dictionary\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    \n",
    "    #Calculate the prediction accuracy\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
    "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class_label\"])/len(data))*100,'%')\n",
    "    \n",
    "\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': {-74: 1.0,\n",
      "        -72: 1.0,\n",
      "        -71: {'x2': {-62.0: 1.0, -59.0: 4.0}},\n",
      "        -70: {'x2': {-64.0: 1.0, -62.0: 1.0, -59.0: 4.0}},\n",
      "        -69: {'x3': {-70.0: 1.0,\n",
      "                     -59.0: 1.0,\n",
      "                     -56.0: 1.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -46.0: 4.0}},\n",
      "        -68: {'x3': {-73.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -63.0: 1.0,\n",
      "                     -62.0: 1.0,\n",
      "                     -61.0: 1.0,\n",
      "                     -60.0: 1.0,\n",
      "                     -59.0: 1.0,\n",
      "                     -54.0: 4.0}},\n",
      "        -67: {'x3': {-68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -63.0: 1.0,\n",
      "                     -62.0: 1.0,\n",
      "                     -60.0: 1.0,\n",
      "                     -59.0: 1.0,\n",
      "                     -56.0: 4.0,\n",
      "                     -55.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -46.0: 4.0}},\n",
      "        -66: {'x5': {-79.0: 1.0,\n",
      "                     -78.0: 1.0,\n",
      "                     -77.0: 1.0,\n",
      "                     -76.0: 1.0,\n",
      "                     -75.0: 1.0,\n",
      "                     -74.0: 1.0,\n",
      "                     -73.0: 1.0,\n",
      "                     -70.0: 1.0,\n",
      "                     -69.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -59.0: 4.0,\n",
      "                     -52.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0}},\n",
      "        -65: {'x5': {-89.0: 1.0,\n",
      "                     -81.0: 1.0,\n",
      "                     -80.0: 1.0,\n",
      "                     -77.0: 1.0,\n",
      "                     -76.0: 1.0,\n",
      "                     -75.0: 1.0,\n",
      "                     -74.0: 1.0,\n",
      "                     -73.0: 1.0,\n",
      "                     -72.0: 1.0,\n",
      "                     -71.0: 1.0,\n",
      "                     -70.0: 1.0,\n",
      "                     -69.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -63.0: 1.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -45.0: 4.0,\n",
      "                     -44.0: 4.0,\n",
      "                     -43.0: 4.0}},\n",
      "        -64: {'x5': {-80.0: 1.0,\n",
      "                     -77.0: 1.0,\n",
      "                     -76.0: 1.0,\n",
      "                     -75.0: 1.0,\n",
      "                     -74.0: 1.0,\n",
      "                     -73.0: 1.0,\n",
      "                     -72.0: 1.0,\n",
      "                     -71.0: 1.0,\n",
      "                     -70.0: 1.0,\n",
      "                     -69.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -53.0: 4.0,\n",
      "                     -52.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -46.0: 4.0,\n",
      "                     -43.0: 4.0,\n",
      "                     -42.0: 4.0,\n",
      "                     -40.0: 4.0,\n",
      "                     -39.0: 4.0}},\n",
      "        -63: {'x5': {-78.0: 1.0,\n",
      "                     -77.0: 1.0,\n",
      "                     -76.0: 1.0,\n",
      "                     -75.0: 1.0,\n",
      "                     -74.0: 1.0,\n",
      "                     -73.0: 1.0,\n",
      "                     -72.0: 1.0,\n",
      "                     -71.0: 1.0,\n",
      "                     -70.0: 1.0,\n",
      "                     -69.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -59.0: 3.0,\n",
      "                     -52.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -46.0: 4.0,\n",
      "                     -45.0: 4.0,\n",
      "                     -44.0: 4.0,\n",
      "                     -43.0: 4.0,\n",
      "                     -42.0: 4.0}},\n",
      "        -62: {'x5': {-80.0: 1.0,\n",
      "                     -76.0: 1.0,\n",
      "                     -75.0: 1.0,\n",
      "                     -74.0: 1.0,\n",
      "                     -73.0: 1.0,\n",
      "                     -72.0: 1.0,\n",
      "                     -71.0: 1.0,\n",
      "                     -70.0: 1.0,\n",
      "                     -69.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -63.0: 1.0,\n",
      "                     -62.0: 1.0,\n",
      "                     -60.0: 1.0,\n",
      "                     -54.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -46.0: 4.0,\n",
      "                     -45.0: 4.0,\n",
      "                     -44.0: 4.0,\n",
      "                     -42.0: 4.0,\n",
      "                     -41.0: 4.0}},\n",
      "        -61: {'x5': {-77.0: 1.0,\n",
      "                     -75.0: 1.0,\n",
      "                     -74.0: 1.0,\n",
      "                     -73.0: 1.0,\n",
      "                     -72.0: 1.0,\n",
      "                     -71.0: 1.0,\n",
      "                     -70.0: 1.0,\n",
      "                     -69.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -63.0: 1.0,\n",
      "                     -56.0: 4.0,\n",
      "                     -55.0: 4.0,\n",
      "                     -54.0: 4.0,\n",
      "                     -53.0: 4.0,\n",
      "                     -52.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -46.0: 4.0,\n",
      "                     -45.0: 4.0,\n",
      "                     -44.0: 4.0,\n",
      "                     -43.0: 4.0,\n",
      "                     -42.0: 4.0}},\n",
      "        -60: {'x5': {-76.0: 1.0,\n",
      "                     -75.0: 1.0,\n",
      "                     -73.0: 1.0,\n",
      "                     -72.0: 1.0,\n",
      "                     -71.0: {'x2': {-62.0: 1.0, -50.0: 3.0}},\n",
      "                     -70.0: 1.0,\n",
      "                     -69.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -62.0: 1.0,\n",
      "                     -60.0: 3.0,\n",
      "                     -54.0: 4.0,\n",
      "                     -53.0: 4.0,\n",
      "                     -52.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -46.0: 4.0,\n",
      "                     -45.0: 4.0,\n",
      "                     -42.0: 4.0,\n",
      "                     -39.0: 4.0}},\n",
      "        -59: {'x5': {-76.0: 1.0,\n",
      "                     -74.0: 1.0,\n",
      "                     -73.0: 1.0,\n",
      "                     -69.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -63.0: 1.0,\n",
      "                     -62.0: 1.0,\n",
      "                     -58.0: 4.0,\n",
      "                     -56.0: 4.0,\n",
      "                     -55.0: 4.0,\n",
      "                     -54.0: 4.0,\n",
      "                     -53.0: 4.0,\n",
      "                     -52.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -46.0: 4.0,\n",
      "                     -45.0: 4.0,\n",
      "                     -44.0: 4.0,\n",
      "                     -43.0: 4.0,\n",
      "                     -42.0: 4.0}},\n",
      "        -58: {'x5': {-75.0: 1.0,\n",
      "                     -74.0: 1.0,\n",
      "                     -72.0: 1.0,\n",
      "                     -70.0: 1.0,\n",
      "                     -69.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -61.0: 1.0,\n",
      "                     -55.0: 4.0,\n",
      "                     -54.0: 4.0,\n",
      "                     -53.0: 4.0,\n",
      "                     -52.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -44.0: 4.0,\n",
      "                     -43.0: 4.0,\n",
      "                     -36.0: 4.0}},\n",
      "        -57: {'x5': {-77.0: 1.0,\n",
      "                     -75.0: 1.0,\n",
      "                     -73.0: 1.0,\n",
      "                     -71.0: {'x2': {-58.0: 3.0, -53.0: 1.0}},\n",
      "                     -70.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -66.0: 1.0,\n",
      "                     -65.0: 1.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -62.0: 1.0,\n",
      "                     -56.0: 4.0,\n",
      "                     -54.0: 4.0,\n",
      "                     -53.0: 4.0,\n",
      "                     -52.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -46.0: 4.0,\n",
      "                     -44.0: 4.0,\n",
      "                     -43.0: 4.0}},\n",
      "        -56: {'x5': {-74.0: 1.0,\n",
      "                     -72.0: 1.0,\n",
      "                     -71.0: {'x2': {-53.0: 1.0, -51.0: 3.0}},\n",
      "                     -70.0: 1.0,\n",
      "                     -68.0: 1.0,\n",
      "                     -67.0: 1.0,\n",
      "                     -64.0: 3.0,\n",
      "                     -61.0: 1.0,\n",
      "                     -58.0: 4.0,\n",
      "                     -57.0: 3.0,\n",
      "                     -55.0: 4.0,\n",
      "                     -53.0: 4.0,\n",
      "                     -52.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -47.0: 4.0,\n",
      "                     -46.0: 4.0,\n",
      "                     -43.0: 4.0}},\n",
      "        -55: {'x5': {-70.0: 3.0,\n",
      "                     -64.0: 1.0,\n",
      "                     -63.0: 3.0,\n",
      "                     -60.0: 3.0,\n",
      "                     -58.0: 3.0,\n",
      "                     -56.0: 4.0,\n",
      "                     -55.0: 4.0,\n",
      "                     -53.0: 4.0,\n",
      "                     -51.0: 4.0,\n",
      "                     -50.0: 4.0,\n",
      "                     -49.0: 4.0,\n",
      "                     -48.0: 4.0,\n",
      "                     -46.0: 4.0,\n",
      "                     -45.0: 4.0}},\n",
      "        -54: {'x2': {-61.0: 3.0,\n",
      "                     -59.0: 3.0,\n",
      "                     -57.0: 4.0,\n",
      "                     -56.0: 3.0,\n",
      "                     -55.0: 3.0,\n",
      "                     -54.0: 3.0,\n",
      "                     -53.0: 3.0,\n",
      "                     -52.0: 3.0,\n",
      "                     -46.0: 4.0}},\n",
      "        -53: 3.0,\n",
      "        -52: {'x3': {-65.0: 2.0,\n",
      "                     -61.0: 3.0,\n",
      "                     -59.0: 3.0,\n",
      "                     -58.0: 3.0,\n",
      "                     -57.0: 3.0,\n",
      "                     -56.0: 3.0,\n",
      "                     -55.0: 3.0,\n",
      "                     -54.0: 3.0,\n",
      "                     -53.0: 3.0,\n",
      "                     -52.0: 3.0,\n",
      "                     -51.0: 3.0,\n",
      "                     -50.0: 3.0,\n",
      "                     -49.0: 3.0,\n",
      "                     -47.0: 3.0}},\n",
      "        -51: {'x4': {-58.0: 3.0,\n",
      "                     -57.0: 3.0,\n",
      "                     -56.0: 3.0,\n",
      "                     -55.0: 3.0,\n",
      "                     -54.0: 3.0,\n",
      "                     -53.0: 3.0,\n",
      "                     -52.0: 3.0,\n",
      "                     -51.0: 3.0,\n",
      "                     -50.0: 3.0,\n",
      "                     -49.0: 3.0,\n",
      "                     -48.0: 3.0,\n",
      "                     -47.0: 3.0,\n",
      "                     -46.0: 3.0,\n",
      "                     -45.0: 2.0}},\n",
      "        -50: {'x5': {-82.0: 2.0,\n",
      "                     -80.0: 2.0,\n",
      "                     -76.0: 2.0,\n",
      "                     -74.0: 2.0,\n",
      "                     -73.0: 2.0,\n",
      "                     -72.0: 3.0,\n",
      "                     -71.0: {'x3': {-63.0: 2.0, -54.0: 3.0}},\n",
      "                     -70.0: 2.0,\n",
      "                     -69.0: 3.0,\n",
      "                     -68.0: 3.0,\n",
      "                     -67.0: 3.0,\n",
      "                     -66.0: 3.0,\n",
      "                     -65.0: 3.0,\n",
      "                     -64.0: 3.0,\n",
      "                     -63.0: {'x2': {-68.0: 3.0, -56.0: 2.0, -54.0: 3.0}},\n",
      "                     -62.0: 3.0,\n",
      "                     -61.0: 3.0,\n",
      "                     -60.0: 3.0,\n",
      "                     -58.0: 3.0,\n",
      "                     -54.0: 3.0}},\n",
      "        -49: {'x3': {-61.0: 3.0,\n",
      "                     -60.0: 2.0,\n",
      "                     -59.0: 2.0,\n",
      "                     -58.0: {'x2': {-64.0: 3.0, -56.0: 2.0, -51.0: 3.0}},\n",
      "                     -57.0: 3.0,\n",
      "                     -56.0: 3.0,\n",
      "                     -55.0: 3.0,\n",
      "                     -53.0: 3.0,\n",
      "                     -52.0: 3.0,\n",
      "                     -51.0: 3.0,\n",
      "                     -50.0: 3.0,\n",
      "                     -49.0: 3.0,\n",
      "                     -48.0: 3.0,\n",
      "                     -45.0: 3.0}},\n",
      "        -48: {'x4': {-59.0: 3.0,\n",
      "                     -56.0: 3.0,\n",
      "                     -55.0: 3.0,\n",
      "                     -54.0: 3.0,\n",
      "                     -53.0: 3.0,\n",
      "                     -52.0: 3.0,\n",
      "                     -51.0: 3.0,\n",
      "                     -50.0: 3.0,\n",
      "                     -49.0: {'x2': {-57.0: 2.0, -54.0: 3.0}},\n",
      "                     -48.0: 3.0,\n",
      "                     -47.0: 3.0,\n",
      "                     -46.0: {'x2': {-58.0: 3.0, -56.0: 2.0}},\n",
      "                     -45.0: {'x2': {-58.0: 2.0, -53.0: 3.0}},\n",
      "                     -44.0: 2.0,\n",
      "                     -43.0: 3.0,\n",
      "                     -42.0: 2.0,\n",
      "                     -39.0: 2.0,\n",
      "                     -38.0: 2.0,\n",
      "                     -37.0: 2.0,\n",
      "                     -36.0: 2.0}},\n",
      "        -47: {'x7': {-88.0: 3.0,\n",
      "                     -87.0: 3.0,\n",
      "                     -86.0: 3.0,\n",
      "                     -85.0: {'x2': {-60.0: 3.0,\n",
      "                                    -58.0: 2.0,\n",
      "                                    -56.0: 3.0,\n",
      "                                    -55.0: 3.0,\n",
      "                                    -53.0: 3.0}},\n",
      "                     -84.0: 3.0,\n",
      "                     -83.0: 3.0,\n",
      "                     -82.0: 3.0,\n",
      "                     -81.0: 2.0,\n",
      "                     -80.0: 3.0,\n",
      "                     -79.0: 3.0,\n",
      "                     -77.0: 3.0,\n",
      "                     -75.0: 3.0,\n",
      "                     -74.0: 3.0,\n",
      "                     -71.0: 2.0}},\n",
      "        -46: {'x5': {-74.0: 2.0,\n",
      "                     -71.0: 2.0,\n",
      "                     -68.0: 3.0,\n",
      "                     -67.0: 2.0,\n",
      "                     -66.0: 3.0,\n",
      "                     -65.0: {'x2': {-57.0: 2.0, -47.0: 3.0}},\n",
      "                     -64.0: 3.0,\n",
      "                     -63.0: 3.0,\n",
      "                     -62.0: {'x6': {-85.0: 3.0,\n",
      "                                    -81.0: 3.0,\n",
      "                                    -80.0: 3.0,\n",
      "                                    -79.0: 3.0,\n",
      "                                    -77.0: 3.0,\n",
      "                                    -76.0: 2.0}},\n",
      "                     -61.0: 3.0,\n",
      "                     -60.0: 3.0,\n",
      "                     -59.0: 3.0,\n",
      "                     -58.0: 3.0,\n",
      "                     -57.0: 3.0}},\n",
      "        -45: {'x5': {-80.0: 2.0,\n",
      "                     -78.0: 2.0,\n",
      "                     -77.0: 2.0,\n",
      "                     -73.0: 2.0,\n",
      "                     -71.0: 2.0,\n",
      "                     -70.0: 2.0,\n",
      "                     -69.0: 2.0,\n",
      "                     -68.0: {'x2': {-58.0: 2.0, -56.0: 3.0, -54.0: 3.0}},\n",
      "                     -67.0: {'x2': {-63.0: 3.0, -57.0: 2.0, -53.0: 3.0}},\n",
      "                     -66.0: 2.0,\n",
      "                     -65.0: {'x2': {-58.0: 2.0, -54.0: 3.0}},\n",
      "                     -64.0: 3.0,\n",
      "                     -63.0: 3.0,\n",
      "                     -62.0: 3.0,\n",
      "                     -61.0: 3.0,\n",
      "                     -59.0: 3.0,\n",
      "                     -58.0: 3.0,\n",
      "                     -57.0: 3.0}},\n",
      "        -44: {'x4': {-52.0: 3.0,\n",
      "                     -49.0: 3.0,\n",
      "                     -48.0: 3.0,\n",
      "                     -47.0: 2.0,\n",
      "                     -46.0: 2.0,\n",
      "                     -45.0: 2.0,\n",
      "                     -41.0: 2.0,\n",
      "                     -36.0: 2.0}},\n",
      "        -43: {'x4': {-52.0: 3.0,\n",
      "                     -47.0: 2.0,\n",
      "                     -46.0: 2.0,\n",
      "                     -45.0: 2.0,\n",
      "                     -44.0: 2.0,\n",
      "                     -43.0: 2.0,\n",
      "                     -42.0: 2.0,\n",
      "                     -41.0: 2.0,\n",
      "                     -40.0: 2.0,\n",
      "                     -39.0: 2.0,\n",
      "                     -38.0: 2.0,\n",
      "                     -37.0: 2.0,\n",
      "                     -35.0: 2.0}},\n",
      "        -42: {'x3': {-65.0: 2.0,\n",
      "                     -63.0: 2.0,\n",
      "                     -62.0: 2.0,\n",
      "                     -61.0: 2.0,\n",
      "                     -58.0: 2.0,\n",
      "                     -57.0: 2.0,\n",
      "                     -56.0: 2.0,\n",
      "                     -55.0: 2.0,\n",
      "                     -54.0: 2.0,\n",
      "                     -53.0: 2.0,\n",
      "                     -52.0: 2.0,\n",
      "                     -51.0: 2.0,\n",
      "                     -50.0: 2.0,\n",
      "                     -49.0: 2.0,\n",
      "                     -48.0: 3.0}},\n",
      "        -41: 2.0,\n",
      "        -40: 2.0,\n",
      "        -39: 2.0,\n",
      "        -38: 2.0,\n",
      "        -37: 2.0,\n",
      "        -36: 2.0,\n",
      "        -35: 2.0,\n",
      "        -34: 2.0,\n",
      "        -27: 2.0,\n",
      "        -26: 2.0,\n",
      "        -21: 2.0,\n",
      "        -20: 2.0,\n",
      "        -19: 2.0,\n",
      "        -18: 2.0,\n",
      "        -17: 2.0,\n",
      "        -16: 2.0,\n",
      "        -15: 2.0,\n",
      "        -14: 2.0,\n",
      "        -13: 2.0,\n",
      "        -12: 2.0,\n",
      "        -11: 2.0,\n",
      "        -10: 2.0}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is:  88.0 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the tree, Print the tree and predict the accuracy\n",
    "\"\"\"\n",
    "training_data = train_test_split(dataset)[0]\n",
    "testing_data = train_test_split(dataset)[1] \n",
    "\n",
    "X_train = training_data.drop(\"class_label\", axis=1)\n",
    "X_test = testing_data.drop(\"class_label\", axis=1)\n",
    "\n",
    "y_train = training_data[\"class_label\"]\n",
    "y_test = testing_data[\"class_label\"]\n",
    "\n",
    "tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
    "pprint(tree)\n",
    "predicted_custom_dt = test(testing_data,tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predicted_custom_dt[\"predicted\"] == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      2\n",
       "2      3\n",
       "3      3\n",
       "4      3\n",
       "      ..\n",
       "395    1\n",
       "396    3\n",
       "397    4\n",
       "398    3\n",
       "399    4\n",
       "Name: class_label, Length: 400, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      2\n",
       "2      3\n",
       "3      3\n",
       "4      3\n",
       "      ..\n",
       "395    1\n",
       "396    3\n",
       "397    4\n",
       "398    3\n",
       "399    4\n",
       "Name: class_label, Length: 400, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data[\"class_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      2\n",
       "2      3\n",
       "3      3\n",
       "4      3\n",
       "      ..\n",
       "395    1\n",
       "396    3\n",
       "397    4\n",
       "398    3\n",
       "399    4\n",
       "Name: class_label, Length: 400, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted\n",
       "0           2\n",
       "1           2\n",
       "2           3\n",
       "3           3\n",
       "4           3\n",
       "..        ...\n",
       "395         1\n",
       "396         3\n",
       "397         4\n",
       "398         3\n",
       "399         4\n",
       "\n",
       "[400 rows x 1 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_custom_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************************************************************\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.98      0.82       103\n",
      "           2       0.98      0.91      0.94        92\n",
      "           3       0.97      0.80      0.87       108\n",
      "           4       0.99      0.84      0.91        97\n",
      "\n",
      "    accuracy                           0.88       400\n",
      "   macro avg       0.91      0.88      0.89       400\n",
      "weighted avg       0.91      0.88      0.88       400\n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Accuracy : 0.88\n",
      "**********************************************************************************************************************************\n",
      "Confusion Matrix : \n",
      " [[101   0   2   0]\n",
      " [  7  84   1   0]\n",
      " [ 19   2  86   1]\n",
      " [ 16   0   0  81]]\n",
      "**********************************************************************************************************************************\n",
      "Precision for class 0 : 0.7062937062937062\n",
      "Precision for class 1 : 0.9767441860465116\n",
      "Precision for class 2 : 0.9662921348314607\n",
      "Precision for class 3 : 0.9878048780487805\n",
      "**********************************************************************************************************************************\n",
      "Sensitivity for class 0 : 0.9805825242718447\n",
      "Sensitivity for class 1 : 0.9130434782608695\n",
      "Sensitivity for class 2 : 0.7962962962962963\n",
      "Sensitivity for class 3 : 0.8350515463917526\n",
      "**********************************************************************************************************************************\n",
      "Specificity for class 0 : 0.8585858585858586\n",
      "Specificity for class 1 : 0.9935064935064936\n",
      "Specificity for class 2 : 0.9897260273972602\n",
      "Specificity for class 3 : 0.9966996699669967\n",
      "**********************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "y_pred_custom = predicted_custom_dt[\"predicted\"].to_numpy()\n",
    "y_pred_custom = y_pred_custom.astype(int)\n",
    "model_evaluation(y_test, y_pred_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data.drop(\"class_label\", axis=1)\n",
    "X_test = testing_data.drop(\"class_label\", axis=1)\n",
    "\n",
    "y_train = training_data[\"class_label\"]\n",
    "y_test = testing_data[\"class_label\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      2\n",
       "2      3\n",
       "3      3\n",
       "4      4\n",
       "      ..\n",
       "395    4\n",
       "396    1\n",
       "397    4\n",
       "398    1\n",
       "399    4\n",
       "Name: class_label, Length: 400, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import multilabel_confusion_matrix, plot_confusion_matrix, roc_auc_score\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from itertools import cycle\n",
    "\n",
    "# Setting a RANDOM SEED for consistant result(deterministic random data) \n",
    "RANDOM_SEED=4\n",
    "print_divider = \"*\"* 130\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_classifier(X_train, y_train, crit='gini'):\n",
    "    '''\n",
    "    X_train: Input features \n",
    "    y_train: Class label for the training data(X_train)\n",
    "    criterion: default is \"gini\", it could be \"entropy\" as well\n",
    "\n",
    "    returns:\n",
    "    clf_tree: df_classifier object \n",
    "    '''\n",
    "    clf_tree = DecisionTreeClassifier(criterion=crit)\n",
    "    clf_tree.fit(X_train, y_train)\n",
    "\n",
    "    return clf_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(y_test, y_pred):\n",
    "    # Accuracy \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Confusion Matrix \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(print_divider)\n",
    "    # Precision and Recall (Classification Report) \n",
    "    clf_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report\")\n",
    "    print(clf_report)\n",
    "    print(print_divider)\n",
    "\n",
    "    # Sensitivity and Specificity\n",
    "    mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    tn = mcm[:, 0, 0]\n",
    "    tp = mcm[:, 1, 1]\n",
    "    fn = mcm[:, 1, 0]\n",
    "    fp = mcm[:, 0, 1]\n",
    "    sensitivity = tp / (tp + fn) \n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    print(f\"Accuracy : {accuracy}\")\n",
    "    print(print_divider)\n",
    "    print(f\"Confusion Matrix : \\n {cm}\")\n",
    "    print(print_divider)\n",
    "    for c in range(len(precision)):\n",
    "        print(f\"Precision for class {c} : {precision[c]}\")\n",
    "    print(print_divider)\n",
    "    for c in range(len(sensitivity)):\n",
    "        print(f\"Sensitivity for class {c} : {sensitivity[c]}\")\n",
    "    print(print_divider)\n",
    "    for c in range(len(specificity)):\n",
    "        print(f\"Specificity for class {c} : {specificity[c]}\")\n",
    "    print(print_divider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data.drop(\"class_label\", axis=1)\n",
    "X_test = testing_data.drop(\"class_label\", axis=1)\n",
    "\n",
    "y_train = training_data[\"class_label\"]\n",
    "y_test = testing_data[\"class_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = dt_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************************************************************\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      0.99      0.99       102\n",
      "           2       0.97      0.95      0.96       100\n",
      "           3       0.94      0.95      0.94        98\n",
      "           4       0.99      1.00      1.00       100\n",
      "\n",
      "    accuracy                           0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Accuracy : 0.9725\n",
      "**********************************************************************************************************************************\n",
      "Confusion Matrix : \n",
      " [[101   0   1   0]\n",
      " [  0  95   5   0]\n",
      " [  1   3  93   1]\n",
      " [  0   0   0 100]]\n",
      "**********************************************************************************************************************************\n",
      "Precision for class 0 : 0.9901960784313726\n",
      "Precision for class 1 : 0.9693877551020408\n",
      "Precision for class 2 : 0.9393939393939394\n",
      "Precision for class 3 : 0.9900990099009901\n",
      "**********************************************************************************************************************************\n",
      "Sensitivity for class 0 : 0.9901960784313726\n",
      "Sensitivity for class 1 : 0.95\n",
      "Sensitivity for class 2 : 0.9489795918367347\n",
      "Sensitivity for class 3 : 1.0\n",
      "**********************************************************************************************************************************\n",
      "Specificity for class 0 : 0.9966442953020134\n",
      "Specificity for class 1 : 0.99\n",
      "Specificity for class 2 : 0.9801324503311258\n",
      "Specificity for class 3 : 0.9966666666666667\n",
      "**********************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "model_evaluation(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************************************************************\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.21      0.28      0.24       103\n",
      "           2       0.33      0.31      0.32        98\n",
      "           3       0.27      0.25      0.26       100\n",
      "           4       0.25      0.20      0.22        99\n",
      "\n",
      "    accuracy                           0.26       400\n",
      "   macro avg       0.27      0.26      0.26       400\n",
      "weighted avg       0.26      0.26      0.26       400\n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Accuracy : 0.26\n",
      "**********************************************************************************************************************************\n",
      "Confusion Matrix : \n",
      " [[29 25 27 22]\n",
      " [33 30 17 18]\n",
      " [37 18 25 20]\n",
      " [37 19 23 20]]\n",
      "**********************************************************************************************************************************\n",
      "Precision for class 0 : 0.21323529411764705\n",
      "Precision for class 1 : 0.32608695652173914\n",
      "Precision for class 2 : 0.2717391304347826\n",
      "Precision for class 3 : 0.25\n",
      "**********************************************************************************************************************************\n",
      "Sensitivity for class 0 : 0.2815533980582524\n",
      "Sensitivity for class 1 : 0.30612244897959184\n",
      "Sensitivity for class 2 : 0.25\n",
      "Sensitivity for class 3 : 0.20202020202020202\n",
      "**********************************************************************************************************************************\n",
      "Specificity for class 0 : 0.6397306397306397\n",
      "Specificity for class 1 : 0.7947019867549668\n",
      "Specificity for class 2 : 0.7766666666666666\n",
      "Specificity for class 3 : 0.8006644518272426\n",
      "**********************************************************************************************************************************\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      1\n",
       "2      2\n",
       "3      4\n",
       "4      3\n",
       "      ..\n",
       "395    4\n",
       "396    4\n",
       "397    4\n",
       "398    2\n",
       "399    1\n",
       "Name: class_label, Length: 400, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      3\n",
       "2      3\n",
       "3      4\n",
       "4      1\n",
       "      ..\n",
       "395    2\n",
       "396    2\n",
       "397    1\n",
       "398    4\n",
       "399    4\n",
       "Name: predicted, Length: 400, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pred_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 4, 1, 1, 2, 4, 4, 2, 1, 3, 1, 1, 3, 1, 4, 1, 1, 1, 1, 1,\n",
       "       4, 1, 3, 1, 1, 2, 1, 1, 1, 1, 4, 2, 1, 1, 4, 3, 1, 3, 1, 3, 1, 1,\n",
       "       1, 3, 4, 2, 3, 1, 1, 3, 1, 1, 1, 3, 4, 1, 1, 2, 2, 1, 3, 2, 1, 3,\n",
       "       4, 4, 2, 1, 2, 1, 4, 4, 2, 4, 4, 4, 4, 4, 1, 4, 1, 4, 2, 1, 1, 1,\n",
       "       1, 3, 3, 1, 1, 1, 1, 4, 1, 3, 2, 1, 1, 2, 2, 1, 2, 1, 4, 1, 4, 2,\n",
       "       2, 1, 3, 1, 3, 2, 2, 4, 4, 4, 1, 3, 3, 3, 4, 1, 2, 3, 3, 4, 1, 1,\n",
       "       2, 3, 4, 4, 3, 4, 1, 4, 4, 2, 3, 3, 4, 2, 4, 1, 3, 3, 4, 3, 4, 3,\n",
       "       4, 4, 2, 2, 3, 4, 3, 2, 1, 2, 4, 3, 3, 2, 1, 2, 3, 3, 4, 3, 3, 2,\n",
       "       4, 1, 1, 1, 1, 4, 1, 2, 3, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3,\n",
       "       3, 3, 2, 3, 1, 4, 3, 4, 2, 3, 1, 4, 1, 1, 2, 2, 4, 1, 3, 2, 2, 4,\n",
       "       2, 2, 2, 4, 2, 4, 1, 4, 1, 4, 1, 3, 1, 3, 3, 1, 2, 4, 1, 4, 4, 1,\n",
       "       1, 4, 1, 2, 3, 2, 1, 2, 3, 1, 3, 3, 3, 4, 2, 2, 2, 3, 1, 3, 4, 3,\n",
       "       2, 2, 4, 1, 3, 1, 1, 1, 4, 2, 3, 4, 3, 3, 2, 2, 2, 1, 3, 1, 3, 1,\n",
       "       1, 2, 3, 1, 1, 4, 1, 2, 2, 1, 4, 2, 3, 1, 1, 4, 1, 1, 4, 2, 4, 3,\n",
       "       3, 2, 1, 2, 1, 1, 2, 1, 2, 4, 2, 1, 3, 1, 3, 3, 1, 1, 1, 4, 3, 1,\n",
       "       3, 1, 1, 3, 2, 2, 2, 4, 2, 1, 2, 3, 1, 2, 1, 2, 2, 3, 4, 2, 1, 2,\n",
       "       1, 2, 3, 1, 2, 3, 4, 2, 3, 1, 1, 1, 3, 1, 1, 1, 3, 4, 4, 2, 3, 1,\n",
       "       1, 2, 4, 3, 1, 1, 1, 2, 4, 3, 4, 4, 1, 3, 2, 3, 3, 3, 3, 1, 3, 2,\n",
       "       2, 1, 4, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_custom.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0, 3.0, 3.0, 4.0, 1, 1, 2.0, 4.0, 4.0, 2.0, 1, 3.0, 1.0, 1.0,\n",
       "       3.0, 1.0, 4.0, 1, 1.0, 1, 1, 1.0, 4.0, 1.0, 3.0, 1.0, 1, 2.0, 1.0,\n",
       "       1, 1.0, 1.0, 4.0, 2.0, 1.0, 1.0, 4.0, 3.0, 1.0, 3.0, 1.0, 3.0, 1.0,\n",
       "       1.0, 1.0, 3.0, 4.0, 2.0, 3.0, 1.0, 1.0, 3.0, 1, 1, 1, 3.0, 4.0,\n",
       "       1.0, 1.0, 2.0, 2.0, 1, 3.0, 2.0, 1.0, 3.0, 4.0, 4.0, 2.0, 1.0, 2.0,\n",
       "       1.0, 4.0, 4.0, 2.0, 4.0, 4.0, 4.0, 4.0, 4.0, 1, 4.0, 1.0, 4.0, 2.0,\n",
       "       1, 1.0, 1, 1, 3.0, 3.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 3.0, 2.0, 1,\n",
       "       1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 4.0, 1, 4.0, 2.0, 2.0, 1, 3.0, 1.0,\n",
       "       3.0, 2.0, 2.0, 4.0, 4.0, 4.0, 1.0, 3.0, 3.0, 3.0, 4.0, 1, 2.0, 3.0,\n",
       "       3.0, 4.0, 1, 1.0, 2.0, 3.0, 4.0, 4.0, 3.0, 4.0, 1, 4.0, 4.0, 2.0,\n",
       "       3.0, 3.0, 4.0, 2.0, 4.0, 1, 3.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 4.0,\n",
       "       2.0, 2.0, 3.0, 4.0, 3.0, 2.0, 1.0, 2.0, 4.0, 3.0, 3.0, 2.0, 1, 2.0,\n",
       "       3.0, 3.0, 4.0, 3.0, 3.0, 2.0, 4.0, 1.0, 1.0, 1.0, 1, 4.0, 1, 2.0,\n",
       "       3.0, 2.0, 1, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1, 1.0, 3.0, 3.0,\n",
       "       3.0, 3.0, 2.0, 3.0, 1.0, 4.0, 3.0, 4.0, 2.0, 3.0, 1, 4.0, 1.0, 1,\n",
       "       2.0, 2.0, 4.0, 1.0, 3.0, 2.0, 2.0, 4.0, 2.0, 2.0, 2.0, 4.0, 2.0,\n",
       "       4.0, 1.0, 4.0, 1.0, 4.0, 1.0, 3.0, 1.0, 3.0, 3.0, 1, 2.0, 4.0, 1.0,\n",
       "       4.0, 4.0, 1.0, 1, 4.0, 1, 2.0, 3.0, 2.0, 1.0, 2.0, 3.0, 1.0, 3.0,\n",
       "       3.0, 3.0, 4.0, 2.0, 2.0, 2.0, 3.0, 1.0, 3.0, 4.0, 3.0, 2.0, 2.0,\n",
       "       4.0, 1.0, 3.0, 1, 1.0, 1, 4.0, 2.0, 3.0, 4.0, 3.0, 3.0, 2.0, 2.0,\n",
       "       2.0, 1.0, 3.0, 1.0, 3.0, 1, 1.0, 2.0, 3.0, 1, 1.0, 4.0, 1.0, 2.0,\n",
       "       2.0, 1.0, 4.0, 2.0, 3.0, 1.0, 1.0, 4.0, 1, 1.0, 4.0, 2.0, 4.0, 3.0,\n",
       "       3.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 4.0, 2.0, 1.0, 3.0,\n",
       "       1.0, 3.0, 3.0, 1.0, 1.0, 1, 4.0, 3.0, 1.0, 3.0, 1.0, 1.0, 3.0, 2.0,\n",
       "       2.0, 2.0, 4.0, 2.0, 1.0, 2.0, 3.0, 1.0, 2.0, 1.0, 2.0, 2.0, 3.0,\n",
       "       4.0, 2.0, 1.0, 2.0, 1, 2.0, 3.0, 1, 2.0, 3.0, 4.0, 2.0, 3.0, 1.0,\n",
       "       1.0, 1.0, 3.0, 1, 1.0, 1.0, 3.0, 4.0, 4.0, 2.0, 3.0, 1.0, 1.0, 2.0,\n",
       "       4.0, 3.0, 1.0, 1.0, 1, 2.0, 4.0, 3.0, 4.0, 4.0, 1.0, 3.0, 2.0, 3.0,\n",
       "       3.0, 3.0, 3.0, 1, 3.0, 2.0, 2.0, 1, 4.0, 4.0], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
